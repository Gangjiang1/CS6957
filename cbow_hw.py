# -*- coding: utf-8 -*-
"""CBOW_HW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wT4s0rzDv6Rud97H63QcPiGuWm1IeJH9
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import glob

def get_files(path):
    """ Returns a list of text files in the 'path' directory.
    Input
    ------------
    path: str or pathlib.Path. Directory path to load files from.

    Output
    -----------
    file_list: List. List of paths to text files
    """
    file_list =  list(glob.glob(f"{path}/*.txt"))
    return file_list

def get_word2ix(path = "vocab.txt"):
    """ Generates a mapping given a vocabulary file.
    Input
    -------------
    path: str or pathlib.Path. Relative path to the vocabulary file.

    Output
    -------------
    word2ix: dict. Dictionary mapping words to unique IDs. Keys are words and
                values are the indices.
    """
    word2ix = {}
    with open(path) as f:
        data = f.readlines()
        for line in data:
            word2ix[line.split("\t")[1].strip()] = int(line.split("\t")[0])

    return word2ix

def process_data(files, context_window, word2ix):
    """ Returns the processed data. Processing involves reading data from
    the files, converting the words to appropriate indices, mapping OOV words
    to the [UNK] token and padding appropriately.
    Inputs
    -----------
    files: List. List of files to be processed. Can be the list
            returned by the `get_files()` method.
    context_window: int. Size of the context window. Size is the amount
            of words considered as context either to the left or right of a word
    word2ix: dict. Mapping from word to a unique index. Can be the dict returned by
                the `get_word2ix` method

    Output
    ----------
    data: List[List[int]]. Each list corresponds to a file and the set of indices
            for the contents of the file.
    """
    data = []
    for file in files:
        file_data = [word2ix["[PAD]"]]*context_window
        with open(file) as f:
            lines = f.readlines()
            for line in lines:
                if line.strip() not in word2ix.keys():
                    file_data.append(word2ix["[UNK]"])
                else:
                    file_data.append(word2ix[line.strip()])

            file_data.extend([word2ix["[PAD]"]]*context_window)
            data.append(file_data.copy())

    return data

files=get_files('train')
context_window = 5
word2ix = get_word2ix(path = "vocab.txt")

len(word2ix)

data = process_data(files, context_window, word2ix)

def create_context_target_pairs(data, context_window):
    context_target_data = []
    for file_data in data:
        for i in range(context_window, len(file_data) - context_window):
            context = file_data[i-context_window:i] + file_data[i+1:i+1+context_window]
            target = file_data[i]
            context_target_data.append((context, target))
    return context_target_data

# context_target_data_list = []
# for i in range(len(data)):
#    context_target_data = create_context_target_pairs(data, context_window)
context_target_data_list = create_context_target_pairs(data, context_window)

# print(context_target_data_list[0])

len(context_target_data_list)

context_target_data_list[0]

class CBOW(nn.Module):

    def __init__(self, vocab_size, embedding_dim):
        super(CBOW, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)
        # #N为128
        # self.linear2 = nn.Linear(32, word_size)

    def forward(self, inputs):
        #embeds = self.embeddings(inputs).view((1, -1))
        embeds = self.embeddings(inputs).sum(dim=1)
        out = self.linear(embeds)
        # out = F.relu(self.linear1(embeds))
        # out = self.linear2(out)
        #probs = F.softmax(out, dim=1)
        return out

from torch.utils.data import DataLoader, TensorDataset

data_list=context_target_data_list

contents = [item[0] for item in data_list]
targets = [item[1] for item in data_list]

contents = torch.tensor(contents, dtype=torch.long)
targets = torch.tensor(targets, dtype=torch.long)

dataset = TensorDataset(contents, targets)

# 创建一个 DataLoader 来划分数据集为小批量
batch_size = 128
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

len(data_loader)

EMBEDDING_DIM = 100

model = CBOW(len(word2ix), EMBEDDING_DIM)

loss_function = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练循环
num_epochs = 10

for epoch in range(num_epochs):
    total_loss = 0
    for content, target in data_loader:
        # 清零梯度
        optimizer.zero_grad()

        # 前向传播
        output = model(content)

        # 计算损失
        loss = loss_function(output, target)

        # 反向传播和优化
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    # 打印每个训练周期的损失
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader)}")

# 使用训练好的嵌入层来获取单词向量
word_vectors = model.embeddings.weight.data

# 可以根据需要保存训练好的模型和词向量

# 指定保存文件的路径
import numpy as np

word_vectors_np = word_vectors.numpy()

# 指定要保存的文件名
file_name = "word_vectors.txt"

# 保存numpy数组到txt文件
with open(file_name, "w") as file:
    for row in word_vectors_np:
        row_str = "\t".join(str(val) for val in row)
        file.write(row_str + "\n")

# 指定包含数据的文件名
file_name = "vocab.txt"

# 创建一个空列表来存储第二列的数据
second_column_data = []

# 打开文件进行读取
with open(file_name, 'r') as file:
    for line in file:
        # 将每行数据拆分为列，并获取第二列的值
        columns = line.strip().split('\t')
        if len(columns) >= 2:
            second_column_data.append(columns[1])

# 现在，second_column_data 包含了来自文件的第二列数据

word_vectors.shape

len(second_column_data)

import torch
import numpy as np

# 假设word_vectors是您的PyTorch张量，second_column_data是您的列表
# 将word_vectors转换为NumPy数组
word_vectors_np = word_vectors.numpy()

# 将second_column_data列表转换为NumPy数组并改变形状以添加为第一列
second_column_np = np.array(second_column_data).reshape(-1, 1)

# 使用NumPy的hstack函数将两个数组水平堆叠
concatenated_data = np.hstack((second_column_np, word_vectors_np))

# 将结果转换回PyTorch张量（如果需要）
# concatenated_tensor = torch.from_numpy(concatenated_data)

# 现在，concatenated_tensor 包含了拼接后的数据，第一列是 second_column_data 的数据

concatenated_data

# 指定要保存的文件名
file_name = "concatenated_data.txt"

# 打开文件进行写入
with open(file_name, 'w') as file:
    for row_data in concatenated_data:
        # 将每行数据转换为字符串并使用制表符分隔
        row_str = '\t'.join(str(value) for value in row_data)
        file.write(row_str + '\n')

word_embeddings = {}
with open('concatenated_data.txt', 'r') as f:
    for line in f:
        parts = line.strip().split()
        word = parts[0]
        embedding = np.array([float(val) for val in parts[1:]])
        word_embeddings[word] = embedding

# Function to calculate cosine similarity
def cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    similarity = dot_product / (norm1 * norm2)
    return similarity

# a. Which pair is closer in the word embedding space?
pairs_a = [['cat', 'tiger'], ['plane', 'human'], ['my', 'mine'], ['happy', 'human'], ['happy', 'cat'], ['king', 'princess'], ['ball', 'racket'], ['good', 'ugly'], ['cat', 'racket'], ['good', 'bad']]

for pair in pairs_a:
    similarity_1 = cosine_similarity(word_embeddings[pair[0]], word_embeddings[pair[1]])
    print(f'Similarity between {pair[0]} and {pair[1]}: {similarity_1}')

# Helper function to find the most similar word
def most_similar(word, word_embeddings, topn=5):
    similarities = {}
    for vocab_word, embedding in word_embeddings.items():
        if vocab_word != word:
            similarities[vocab_word] = cosine_similarity(word, embedding)
    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
    return sorted_similarities[:topn]

# b. Using analogies
analogies_b = [['king', 'queen', 'man'], ['king', 'queen', 'prince'], ['king', 'man', 'queen'], ['woman', 'man', 'princess'], ['prince', 'princess', 'man']]

for analogy in analogies_b:
    word_a, word_b, word_c = analogy
    vector_a = word_embeddings[word_a]
    vector_b = word_embeddings[word_b]
    vector_c = word_embeddings[word_c]
    result_vector = vector_b - vector_a + vector_c

    most_similar_word, similarity = most_similar(result_vector, word_embeddings, topn=1)[0]
    print(f'{word_a}:{word_b}, {word_c}:{most_similar_word}')

import numpy as np

# 你的词嵌入字典，这里假设你已经有了你的word_embeddings
# word_embeddings = {}  # 请用你的实际数据初始化这个字典

# Cosine Similarity Function
def cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    similarity = dot_product / (norm1 * norm2)
    return similarity

# Word Similarity Test
def word_similarity_test(pair1, pair2, word_embeddings):
    similarity1 = cosine_similarity(word_embeddings[pair1[0]], word_embeddings[pair1[1]])
    similarity2 = cosine_similarity(word_embeddings[pair2[0]], word_embeddings[pair2[1]])

    print(f"Similarity between {pair1[0]} and {pair1[1]}: {similarity1}")
    print(f"Similarity between {pair2[0]} and {pair2[1]}: {similarity2}")

# Word Analogy Test
def word_analogy_test(a, b, c, word_embeddings):
    result_vector = word_embeddings[b] - word_embeddings[a] + word_embeddings[c]

    most_similar_word, similarity = most_similar(result_vector, word_embeddings, topn=1)[0]
    print(f'{a}:{b}, then {c}:{most_similar_word}')

# Example Word Similarity Tests
word_similarity_test(["wife", "girl"], ["ancient", "letter"], word_embeddings)
word_similarity_test(["science", "knowledge"], ["number", "subject"], word_embeddings)
word_similarity_test(["girl", "woman"], ["girl", "child"], word_embeddings)
# Example Word Analogy Test
word_analogy_test("cat", "tiger", "dog", word_embeddings)
word_analogy_test("happy", "joyful", "sad", word_embeddings)
word_analogy_test("girl", "woman", "boy", word_embeddings)

### eval embs
!pip install gensim
import argparse
from gensim.models import KeyedVectors
from gensim.test.utils import datapath
import os


def get_eval_stats(emb_file):
    wv = KeyedVectors.load_word2vec_format(datapath(emb_file), binary=False)
    sim = wv.evaluate_word_pairs(os.getcwd()+"wordsim_similarity_goldstandard.txt")
    analogy = wv.evaluate_word_analogies(os.getcwd()+"questions-words_headered.txt")
    print(f"Word Similarity Test Pearson Correlation: {sim[0][0]}")
    print(f"Accuracy on Analogy Test: {analogy[0]}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--emb_file", default=os.getcwd()+"/embeddings.txt", type=str)
    args = vars(parser.parse_args())
    get_eval_stats(args["emb_file"])

